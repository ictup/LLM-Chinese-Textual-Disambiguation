{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfccdeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Coding\\NLP\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-01 15:07:20,686 - INFO: 加载语义模型: intfloat/multilingual-e5-base\n",
      "2025-05-01 15:07:20,689 - INFO: Load pretrained SentenceTransformer: intfloat/multilingual-e5-base\n",
      "2025-05-01 15:07:23,640 - INFO: 开始精确去重...\n",
      "100%|██████████| 1020/1020 [00:00<00:00, 338491.18it/s]\n",
      "2025-05-01 15:07:23,650 - INFO: 开始编码句子...\n",
      "Batches: 100%|██████████| 8/8 [00:04<00:00,  1.79it/s]\n",
      "2025-05-01 15:07:28,118 - INFO: 计算相似度矩阵...\n",
      "扫描句子对: 100%|██████████| 1001/1001 [00:00<00:00, 17057.57it/s]\n",
      "2025-05-01 15:07:28,193 - INFO: \n",
      "===== 处理结果统计 =====\n",
      "2025-05-01 15:07:28,193 - INFO: 原始总行数: 1020\n",
      "2025-05-01 15:07:28,193 - INFO: 精确去重保留: 1001\n",
      "2025-05-01 15:07:28,194 - INFO: 最终保留行数: 948\n",
      "2025-05-01 15:07:28,194 - INFO: 总重复项: 72\n",
      "2025-05-01 15:07:28,195 - INFO: 精确去重结果: d:\\python\\Coding\\NLP\\精确去重结果3.csv\n",
      "2025-05-01 15:07:28,195 - INFO: 重复报告: d:\\python\\Coding\\NLP\\重复报告3.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from typing import List, Dict, Tuple, Set\n",
    "\n",
    "class SemanticDeduplicator:\n",
    "    def __init__(self, \n",
    "                 model_name: str = \"intfloat/multilingual-e5-base\", \n",
    "                 exact_threshold: float = 0.98, \n",
    "                 semantic_threshold: float = 0.95,\n",
    "                 logging_level: int = logging.INFO):\n",
    "        \"\"\"\n",
    "        初始化语义去重器\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): 用于语义编码的模型名称\n",
    "            exact_threshold (float): 精确去重阈值\n",
    "            semantic_threshold (float): 语义去重阈值\n",
    "            logging_level (int): 日志级别\n",
    "        \"\"\"\n",
    "        # 配置日志\n",
    "        logging.basicConfig(\n",
    "            level=logging_level, \n",
    "            format='%(asctime)s - %(levelname)s: %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # 初始化参数\n",
    "        self.exact_threshold = exact_threshold\n",
    "        self.semantic_threshold = semantic_threshold\n",
    "        \n",
    "        # 加载模型\n",
    "        self.logger.info(f\"加载语义模型: {model_name}\")\n",
    "        self.model = SentenceTransformer(\n",
    "            model_name, \n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        self.device = self.model.device\n",
    "\n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        规范化文本\n",
    "        \n",
    "        Args:\n",
    "            text (str): 输入文本\n",
    "        \n",
    "        Returns:\n",
    "            str: 规范化后的文本\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        # 去除标点符号\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "        # 转换为小写并去除首尾空白\n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # 压缩多余空白\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def find_semantic_duplicates(\n",
    "        self, \n",
    "        sentences: List[str], \n",
    "        threshold: float = None\n",
    "    ) -> Tuple[Set[int], List[Tuple[int, int, float]]]:\n",
    "        \"\"\"\n",
    "        查找语义重复句子\n",
    "        \n",
    "        Args:\n",
    "            sentences (List[str]): 句子列表\n",
    "            threshold (float, optional): 相似度阈值\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Set[int], List[Tuple[int, int, float]]]: 重复索引和重复对\n",
    "        \"\"\"\n",
    "        threshold = threshold or self.semantic_threshold\n",
    "        \n",
    "        self.logger.info(\"开始编码句子...\")\n",
    "        embeddings = self.model.encode(\n",
    "            sentences, \n",
    "            batch_size=128, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True, # 方便GPU计算\n",
    "            device=self.device\n",
    "        )# return 2-diimentional tensor\n",
    "        \n",
    "        # 归一化嵌入向量\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        #p=2：使用l2范数归一化，方便计算余弦相似度，dim=1：按行进行归一化 \n",
    "        \n",
    "        self.logger.info(\"计算相似度矩阵...\")\n",
    "        sim_matrix = torch.mm(embeddings, embeddings.T) # 0-1之间的相似度矩阵\n",
    "        \n",
    "        duplicates = set()\n",
    "        duplicate_pairs = []\n",
    "        \n",
    "        # 使用numpy加速计算（cpu中numpy计算速度更快）\n",
    "        # 也可以选择numba和multiprocessing加速，后续有需要再修改\n",
    "        sim_matrix_np = sim_matrix.cpu().numpy()\n",
    "        \n",
    "        #计算对角矩阵乘法减少计算量\n",
    "        for i in tqdm(range(sim_matrix.shape[0]), desc=\"扫描句子对\"):\n",
    "            for j in range(i+1, sim_matrix.shape[1]):\n",
    "                if sim_matrix_np[i, j] > threshold:\n",
    "                    duplicates.add(j)\n",
    "                    duplicate_pairs.append((i, j, sim_matrix_np[i, j].item()))#索引i，j，相似度\n",
    "        \n",
    "        return duplicates, duplicate_pairs\n",
    "\n",
    "    def deduplicate_csv(\n",
    "        self, \n",
    "        input_path: str, \n",
    "        output_path: str, \n",
    "        duplicate_report_path: str, \n",
    "        exact_dedup_path: str,\n",
    "        ambiguity_column: str = '歧义句'\n",
    "    ) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        对CSV文件进行去重处理，生成去重报告（包含精确去重和语义去重）。\n",
    "        \n",
    "        Args:\n",
    "            input_path (str): 输入文件路径\n",
    "            output_path (str): 输出文件路径\n",
    "            duplicate_report_path (str): 重复报告路径\n",
    "            exact_dedup_path (str): 精确去重结果路径\n",
    "            ambiguity_column (str): 包含歧义句的列名\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, int]: 处理统计信息\n",
    "        \"\"\"\n",
    "        # 读取数据\n",
    "        with open(input_path, 'r', encoding='utf-8-sig') as f:\n",
    "            reader = csv.DictReader(f)#按照列名进行访问\n",
    "            original_fieldnames = [col for col in reader.fieldnames if col != '是否删除']\n",
    "            rows = list(reader)\n",
    "        \n",
    "        # 精确去重\n",
    "        seen_exact = {}          # 记录已出现的规范化文本 {clean_text: 原始行索引}\n",
    "        deduped_rows = []        # 去重后保留的行\n",
    "        sentences = []           # 用于语义去重的规范化文本集合\n",
    "        original_indices = []    # 保留行的原始索引\n",
    "        exact_duplicates = []        # 精确去重结果（原格式）\n",
    "        exact_duplicates_report = [] # 精确去重报告（新格式）\n",
    "        \n",
    "        self.logger.info(\"开始精确去重...\")\n",
    "        for idx, row in enumerate(tqdm(rows)):\n",
    "            original_text = row[ambiguity_column]#包含歧义句的列\n",
    "            clean_text = self.normalize_text(original_text)\n",
    "            \n",
    "            #先处理精确去重，如果同样的句子已经出现过，就不再进行语义去重\n",
    "            #格式化的句子没有出现过，就加入到语义去重的句子集合中\n",
    "            if clean_text not in seen_exact:\n",
    "                seen_exact[clean_text] = idx\n",
    "                sentences.append(clean_text)\n",
    "                deduped_rows.append(row)\n",
    "                original_indices.append(idx)\n",
    "                # 记录保留行信息（原格式）\n",
    "                exact_duplicates.append({\n",
    "                    '原始行号': idx + 2,\n",
    "                    '原始文本': original_text,\n",
    "                    '规范化文本': clean_text\n",
    "                })\n",
    "            else:\n",
    "                # 记录精确重复项（新格式）\n",
    "                main_id = seen_exact[clean_text]\n",
    "                exact_duplicates_report.append({\n",
    "                    '主句子行号': main_id + 2,\n",
    "                    '主句子内容': rows[main_id][ambiguity_column],\n",
    "                    '重复句子行号': idx + 2,\n",
    "                    '重复句子内容': original_text,\n",
    "                    '去重方式': \"精确去重\"\n",
    "                })\n",
    "\n",
    "        # 写入精确去重结果（原格式）\n",
    "        with open(exact_dedup_path, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['原始行号', '原始文本', '规范化文本'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(exact_duplicates)\n",
    "\n",
    "        \n",
    "        duplicate_records = exact_duplicates_report.copy()\n",
    "\n",
    "        # 语义去重\n",
    "        semantic_duplicates, dup_pairs = self.find_semantic_duplicates(sentences)\n",
    "        \n",
    "        # 添加语义重复记录\n",
    "        for i, j, similarity in dup_pairs:\n",
    "            main_id = original_indices[i]\n",
    "            dup_id = original_indices[j]\n",
    "            duplicate_records.append({\n",
    "                '主句子行号': main_id + 2,\n",
    "                '主句子内容': rows[main_id][ambiguity_column],\n",
    "                '重复句子行号': dup_id + 2,\n",
    "                '重复句子内容': rows[dup_id][ambiguity_column],\n",
    "                '去重方式': f\"语义去重 (相似度: {similarity:.4f})\"\n",
    "            })\n",
    "        \n",
    "        # 写入重复报告（合并精确+语义）\n",
    "        with open(duplicate_report_path, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['主句子行号','主句子内容','重复句子行号','重复句子内容','去重方式'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(duplicate_records)\n",
    "        \n",
    "        # 标记删除项\n",
    "        exact_deleted = set(range(len(rows))) - set(original_indices)\n",
    "        semantic_deleted = {original_indices[j] for _, j, _ in dup_pairs}\n",
    "        deleted_ids = exact_deleted.union(semantic_deleted)\n",
    "        \n",
    "        # 生成最终结果\n",
    "        output_fieldnames = original_fieldnames + ['是否删除']\n",
    "        with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=output_fieldnames)\n",
    "            writer.writeheader()\n",
    "            for idx, row in enumerate(rows):\n",
    "                row['是否删除'] = '是' if idx in deleted_ids else '否'\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        # 统计信息\n",
    "        stats = {\n",
    "            '原始总行数': len(rows),\n",
    "            '精确去重保留': len(exact_duplicates),\n",
    "            '最终保留行数': len(rows) - len(deleted_ids),\n",
    "            '总重复项': len(deleted_ids)\n",
    "        }\n",
    "        \n",
    "        # 打印统计信息\n",
    "        self.logger.info(\"\\n===== 处理结果统计 =====\")\n",
    "        for key, value in stats.items():\n",
    "            self.logger.info(f\"{key}: {value}\")\n",
    "        \n",
    "        self.logger.info(f\"精确去重结果: {exact_dedup_path}\")\n",
    "        self.logger.info(f\"重复报告: {duplicate_report_path}\")\n",
    "        \n",
    "        return stats\n",
    "\n",
    "def main():\n",
    "    # 设定文件夹路径\n",
    "    folder_path = r\"d:\\python\\Coding\\NLP\"\n",
    "\n",
    "    # 创建去重器实例\n",
    "    deduplicator = SemanticDeduplicator(\n",
    "        model_name=\"intfloat/multilingual-e5-base\", \n",
    "        exact_threshold=0.98, \n",
    "        semantic_threshold=0.95,\n",
    "        logging_level=logging.INFO\n",
    "    )\n",
    "\n",
    "    # 生成完整文件路径\n",
    "    input_path = os.path.join(folder_path, r'D:\\python\\Coding\\NLP\\消歧\\歧义句数据集原始.csv')\n",
    "    output_path = os.path.join(folder_path, \"去重后歧义句数据集.csv\")\n",
    "    duplicate_report_path = os.path.join(folder_path, \"重复报告3.csv\")\n",
    "    exact_dedup_path = os.path.join(folder_path, \"精确去重结果3.csv\")\n",
    "\n",
    "    # 运行去重处理\n",
    "    deduplicator.deduplicate_csv(\n",
    "        input_path,\n",
    "        output_path,\n",
    "        duplicate_report_path,\n",
    "        exact_dedup_path\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259b16dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集列提取工具\n",
      "==================================================\n",
      "正在读取文件: D:\\python\\Coding\\NLP\\去重后歧义句数据集.csv\n",
      "成功读取数据，共 1020 行\n",
      "成功提取数据并保存到: D:\\python\\Coding\\NLP\\提取后的数据集.csv\n",
      "提取了 1020 行和 7 列\n",
      "处理完成!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract_columns(input_file, output_file):\n",
    "    \"\"\"\n",
    "    从输入文件中提取指定的列并保存到输出文件\n",
    "    \n",
    "    参数:\n",
    "    input_file (str): 输入文件路径\n",
    "    output_file (str): 输出文件路径\n",
    "    \"\"\"\n",
    "    print(f\"正在读取文件: {input_file}\")\n",
    "    \n",
    "    # 确定文件类型并读取\n",
    "    file_ext = os.path.splitext(input_file)[1].lower()\n",
    "    \n",
    "    try:\n",
    "        if file_ext == '.csv':\n",
    "            df = pd.read_csv(input_file)\n",
    "        elif file_ext in ['.xlsx', '.xls']:\n",
    "            df = pd.read_excel(input_file)\n",
    "        else:\n",
    "            print(f\"不支持的文件格式: {file_ext}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"成功读取数据，共 {len(df)} 行\")\n",
    "        \n",
    "        # 需要保留的列\n",
    "        keep_columns = [\n",
    "            '歧义句',\n",
    "            '歧义句及上下文',\n",
    "            '歧义文本位置',\n",
    "            '歧义原因（解读选项）',\n",
    "            '歧义句消岐1',\n",
    "            '歧义句消岐2',\n",
    "            '歧义类型'\n",
    "        ]\n",
    "        \n",
    "        # 检查列是否在数据集中存在\n",
    "        missing_columns = [col for col in keep_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"警告：以下列在数据集中不存在: {', '.join(missing_columns)}\")\n",
    "            available_columns = [col for col in keep_columns if col in df.columns]\n",
    "            if not available_columns:\n",
    "                print(\"错误：没有找到任何需要保留的列\")\n",
    "                return False\n",
    "            keep_columns = available_columns\n",
    "            \n",
    "        # 提取需要的列\n",
    "        df_extracted = df[keep_columns]\n",
    "        \n",
    "        # 确定输出文件格式并保存\n",
    "        output_ext = os.path.splitext(output_file)[1].lower()\n",
    "        if output_ext == '.csv':\n",
    "            df_extracted.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        elif output_ext in ['.xlsx', '.xls']:\n",
    "            df_extracted.to_excel(output_file, index=False)\n",
    "        else:\n",
    "            # 默认保存为CSV\n",
    "            if not output_ext:\n",
    "                output_file += '.csv'\n",
    "            df_extracted.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"成功提取数据并保存到: {output_file}\")\n",
    "        print(f\"提取了 {len(df_extracted)} 行和 {len(keep_columns)} 列\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"处理过程中出错: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数，处理用户输入并调用相应的功能\n",
    "    \"\"\"\n",
    "    print(\"数据集列提取工具\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    input_file = r'D:\\python\\Coding\\NLP\\去重后歧义句数据集.csv'\n",
    "    output_file = r'D:\\python\\Coding\\NLP\\提取后的数据集.csv'\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"错误：文件 '{input_file}' 不存在!\")\n",
    "        return\n",
    "    \n",
    "    success = extract_columns(input_file, output_file)\n",
    "    if success:\n",
    "        print(\"处理完成!\")\n",
    "    else:\n",
    "        print(\"处理失败，请检查错误信息。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
